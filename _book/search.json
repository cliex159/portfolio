[{"path":"index.html","id":"about-me","chapter":"About me","heading":"About me","text":"eager--learn college senior majoring Applied Mathematics delivering valuable data analytical functions data retrieval methods.  workplace, considering team-oriented critical thinker intern experience relevant undergraduate coursework, highly motivated join group seasoned data analysts cross-functional teammates anticipate growth chance prove abilities reaching clients fluent English speaker.","code":""},{"path":"index.html","id":"education","chapter":"About me","heading":"Education","text":"\nNguyen Thuong Hien High School Gifted - Specialized Math\nInternational University - Applied Mathematics","code":""},{"path":"index.html","id":"experience","chapter":"About me","heading":"Experience","text":"Majoring Applied Mathematics International University, spent past 2 years immersing big data machine learning DataCamp courses. came across online learning platform sake passing Statistics course flying colors. wish came true DataCamp much Statistics, focused building best learning experience specifically Data Science. Now graduate, accumulate important courses learned far publication. Learning courses, assimilate important tools available data visualizing data modeling Python R. wide range statistical machine learning tools belt, summarized projects website personal portfolio.time, one year experience IELTS Teaching Assistant British Council 2019 6-month experience Intertu IB Math Teacher 2020. value first period spend time native speakers without feeling uncomfortable Vietnamese. Also, got used presentational pace foreigners deliver speech highest efficiency 90 minutes. second period, actually small private class personally tutored one two students inferior performance international high schools District 2. reclaim time got re-assimilate statistical tools mathematical theory approach. wish communication skills wonder describe visualization data modeling speaking writing reports.Currently, writing Graduation Thesis Bachelor’s Degree June. title Thesis “Sale Prediction using Machine Learning Algorithms” find via latest upload. description publication done see can use intertwining Python R make visualization modeling data. , also designing web application course Financial Risk Management 2 teacher university.background, wish work closely team members collectively produce multiple teams’ insights can affect change sake company.","code":""},{"path":"index.html","id":"personal-project","chapter":"About me","heading":"Personal Project","text":"Please view projects next sections.","code":""},{"path":"life-expectancy.html","id":"life-expectancy","chapter":"Life Expectancy","heading":"Life Expectancy","text":"\nproject course “Financial Mathematics” students asked visualize life expectancy chosen country. able scrape datasets perform analysis big data.\n","code":""},{"path":"life-expectancy.html","id":"datasets","chapter":"Life Expectancy","heading":"Datasets","text":"","code":""},{"path":"life-expectancy.html","id":"the-information-table","chapter":"Life Expectancy","heading":"The Information Table","text":"First, shall need create information table observe (1) countries, (2) HTTPS link access mortality.org country dataset (3) range years accordingly. much helpful along way whenever encounter index time datasets.","code":"\n## Information table {.unnumbered}\ncountries =c(\"Australia\", \"Finland\", \"Latvia\", \"Slovenia\", \"Austria\", \"France\", \"Lithuania\", \"Spain\", \"Belarus\", \"Germany\", \"Luxembourg\", \"Sweden\", \"Belgium\", \"Greece\", \"Netherlands\", \"Switzerland\", \"Bulgaria\", \"Hong Kong\", \"New Zealand\", \"Taiwan\", \"Canada\", \"Hungary\", \"Norway\", \"U.K.\", \"Chile\", \"Iceland\", \"Poland\", \"U.S.A.\", \"Croatia\", \"Ireland\", \"Portugal\", \"Ukraine\", \"Czechia\", \"Israel\", \"Republic of Korea\", \"Denmark\", \"Italy\", \"Russia\", \"Estonia\", \"Japan\", \"Slovakia\")\n\ninform=data.frame(country = countries,\n                  http= paste(rep(\"https://www.mortality.org\",41),GET(\"https://www.mortality.org\") %>% \n                                read_html() %>% \n                                html_nodes(xpath='//*[@id = \"countries_table\"]/tr/td/p/a') %>%\n                                html_attr('href'),sep=\"\"),\n                  years=rep(0,41))\n\n## A function to extract years for each dataset\nget_years=function(http){\n  get=GET(http)\n  html <- read_html(get)\n  year=html %>% html_nodes(xpath='//p[contains(text(),\"Life tables\")]/../../../tr[12]/td[2]/p') %>% \n    html_text(trim=TRUE)\n  return(year)\n}\n\n## Adding years to information table.  \ninform$years[1:41]=sapply(inform$http[1:24],get_years,USE.NAMES = FALSE)\nglimpse(inform)#> Rows: 41\n#> Columns: 3\n#> $ country <chr> \"Australia\", \"Finland\", \"Latvia\", \"Slovenia\", \"Austria\", \"Fran…\n#> $ http    <chr> \"https://www.mortality.org/cgi-bin/hmd/country.php?cntr=AUS&le…\n#> $ years   <chr> \"1921  - 2019\", \"1878  - 2020\", \"1959  - 2019\", \"1983  - 2019\"…"},{"path":"life-expectancy.html","id":"scraping-the-data","chapter":"Life Expectancy","heading":"Scraping the Data","text":"Note using Rstudio Cloud yields much quicker result! point, shall HTTPS xpath collect 40 datasets store full_df variable. time-consuming step since conversation dataframe containing 800 thousand entries. Function data_set constructed idea :Create HTTPS connection country data using xpath mortality.org homepage.Use read_table function content argument import txt file R. Since mortality.org requires personal account downloading data provided, shall need input account authenticate argument.Filter needed columns.","code":"\n## A function to scrape data from the website.  \ndata_set= function(i,gender){\n  print(paste(\"Successfully scrape \",i,\":\",inform$country[i]))\n  if(gender=='females'){\n    data=read_table(content(GET(paste(rep(\"https://www.mortality.org\"),GET(inform$http[i]) %>% read_html() %>% html_nodes(xpath='//p[contains(text(),\"Life tables\")]/../../../tr[13]/td[3]/div/a') %>% html_attr(\"href\"),sep=\"\"),authenticate(user=\"dattran.hcmiu@gmail.com\", password=\"1632536609\", type = \"basic\"))),col_names = FALSE)\n  } else {\n    data=read_table(content(GET(paste(rep(\"https://www.mortality.org\"),GET(inform$http[i]) %>% read_html() %>% html_nodes(xpath='//p[contains(text(),\"Life tables\")]/../../../tr[14]/td[3]/div/a') %>% html_attr(\"href\"),sep=\"\"),authenticate(user=\"dattran.hcmiu@gmail.com\", password=\"1632536609\", type = \"basic\"))),col_names = FALSE)\n  }\n  data_3=data[3:nrow(data),1:7]\n  colnames(data_3)=c(\"year\", \"Age\", \"mx\", \"qx\", \"ax\", \"lx\", \"dx\")\n  data_3$country=inform$country[i]\n  data_3\n}\n\n## A loop to combine 2*40 data tables \ntable_combine=function(data_gender,gender){\n  for(i in 1:40){\n    new_data_gender=data.frame(data_gender[i])\n    if(i==1){\n      full_data_gender=new_data_gender\n    } else{\n      full_data_gender=rbind(full_data_gender,new_data_gender)\n    }\n  }\n  if(gender==\"females\"){\n    full_data_gender=full_data_gender %>% mutate(gender=\"females\")\n  } else{\n    full_data_gender=full_data_gender %>% mutate(gender=\"males\")\n  }\n}\n\n## Big data {.unnumbered}\n## Error for 25:Chile data still haven't been explained so we only apply the scrape data function for 40 numbers \nfull_df=rbind(table_combine(lapply(c(1:24,26:41),data_set,\"females\"),\"females\"),\n              table_combine(lapply(c(1:24,26:41),data_set,\"males\"),\"males\"))\n## Transform datatype to numeric \nfull_df=transform(full_df,Age=as.numeric(Age),\n                  qx=as.numeric(qx),\n                  lx=as.numeric(lx),\n                  year=as.numeric(year))\nglimpse(full_df)#> Rows: 816,516\n#> Columns: 9\n#> $ year    <dbl> 1921, 1921, 1921, 1921, 1921, 1921, 1921, 1921, 1921, 1921, 19…\n#> $ Age     <dbl> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n#> $ mx      <chr> \"0.05999\", \"0.01206\", \"0.00578\", \"0.00289\", \"0.00325\", \"0.0025…\n#> $ qx      <dbl> 0.05750, 0.01199, 0.00576, 0.00288, 0.00325, 0.00251, 0.00248,…\n#> $ ax      <chr> \"0.28\", \"0.50\", \"0.50\", \"0.50\", \"0.50\", \"0.50\", \"0.50\", \"0.50\"…\n#> $ lx      <dbl> 100000, 94250, 93120, 92583, 92316, 92016, 91785, 91557, 91391…\n#> $ dx      <chr> \"5750\", \"1130\", \"537\", \"267\", \"300\", \"231\", \"228\", \"166\", \"126…\n#> $ country <chr> \"Australia\", \"Australia\", \"Australia\", \"Australia\", \"Australia…\n#> $ gender  <chr> \"females\", \"females\", \"females\", \"females\", \"females\", \"female…"},{"path":"life-expectancy.html","id":"usa-insights","chapter":"Life Expectancy","heading":"USA Insights","text":"","code":""},{"path":"life-expectancy.html","id":"mortality-rates","chapter":"Life Expectancy","heading":"Mortality rates","text":"following codes compare mortality rates across ages ranging zero one hundred ten.look similar graph exponential function suggested taking logarithm may insights.rate infants locally high newborns, decreases.shows upward hiccup around age eighteen. called accident hump. accident hump often caused increased fatalities car accidents (usually pronounced) males compared females.straightens back , reflecting human aging process.","code":"\n## Filter USA, 2019. line: qx~Age by gender \nusa_2019 = full_df %>% \n         filter(country==\"U.S.A.\",\n                year==2019)\nggplot(usa_2019, \n       aes(x=Age, \n           y=qx,group=1)) +\ngeom_line(col=\"red\") +\nlabs(x=\"Age x\", \n     y= expression(paste(\"Mortality rate \", q[x])),\n     title=\"Mortality rates (U.S.A., 2019)\") +\nfacet_wrap(~gender)\n## Filter USA, 2019. line: log(qx)~Age by gender \nggplot(usa_2019, \n       aes(x=Age, \n           y=log(qx),group=1)) +\ngeom_line(col=\"red\") +\nlabs(x=\"Age x\", \n     y= expression(paste(\"Log Mortality rate \", q[x])), \n     title=\"Log Mortality rates (U.S.A., 2019)\") +\nfacet_wrap(~gender)"},{"path":"life-expectancy.html","id":"survival-probablity","chapter":"Life Expectancy","heading":"Survival probablity","text":"convenient illustrate section using examples. calculate 5-year survival probability 18-year-old. First extract px 1 minus qx stored life table. need survival probability 18-year old 22-year-old. multiply one-year survival probabilities get 5-year survival probability. Using prod() function vector relevant one-year survival probabilities gives result.\n\\[{}_{k}p_{x}=p_x.p_{x+1}...p_{x+k-1}\\]\nAlternatively, evaluate probability division number alive people \\(l_x\\) different ages. rounding errors, calculations lead result!\n\\[{}_{k}p_{x}=\\frac{l_{x+k}}{l_{x}}\\]graph indicates probability many years people still alive age eighteen, example, chance eighteen girl survive fifty years sixty-eight years old ninety percent.","code":"\n## Filter USA,2019, Age>=18 \n## Add column survival \nusa_survival = full_df %>% \n         filter(country==\"U.S.A.\",\n                year==2019,\n                Age>=18) %>% \n         group_by(gender) %>% \n         mutate(survival=cumprod(1-qx),\n                k=0:(n()-1)) %>% \n         ungroup()\n## Line: survival~1:92 (18-110) by gender \nggplot(usa_survival,\n       aes(x=k,y=survival)) +\ngeom_line(col=\"red\") +\nlabs(x=\"k\", \n     y=expression(paste(\"\"[k], \"p\"[18])),\n     title=\"Survival probabilities for people in age 18, 2019, U.S.A.\") +\nfacet_wrap(~gender)"},{"path":"life-expectancy.html","id":"life-expectancy-1","chapter":"Life Expectancy","heading":"Life expectancy","text":"Let’s start future lifetime, \\(K_x\\), number whole years lived (x). probability Kx takes value k probability x-year-old survives k years dies within next year, age x+k. manipulation shows probability difference k (k+1)-year survival probability x-year-old.\n\\[P(K_x=k)={}_{k}p_{x}.q_{x+k}={}_{k}p_{x}-{}_{k+1}p_{x}\\]\ncan verify equivalence empirically example 18-year-old. 5-year deferred mortality probability? first expression apply reasoning 5-year survival probability mortality rate age 23, second expression takes difference 5- 6-year survival probability. expressions lead result!\\[{}_{k}p_{x}.q_{x+k}\\]\\[{}_{k}p_{x}-{}_{k+1}p_{x}\\]Using probability function \\(K_x\\), now straightforward calculate expected value random variable. life expectancy x-year-old, expressed whole years. possible outcome k, multiply k probability \\(K_x\\) realizes outcome. Taking sum results life expectancy. simplification leads simple expression: sum k-year survival probabilities k runs 1 maximum possible value.\\[E[K_x]= \\sum_{k=0}^{\\infty} k \\times P(K_x=k) = \\sum_{k=0}^{\\infty} k \\times ({}_{k}p_{x}-{}_{k+1}p_{x}) =  \\sum_{k=1}^{\\infty}{}_{k}p_{x}\\]","code":"\n## Function to compute the curtate expected future lifetime for a given age and life table \ncurtate_future_lifetime <- function(age, life_table) {\n  px <- 1-life_table$qx\n  kpx <- cumprod(px[(age+1):length(px)])\n  sum(kpx)\n}\n\n## Vector of ages \nages <- (full_df %>% \n           filter(country==\"U.S.A.\",\n                  year==2019,\n                  gender==\"females\") %>% \n           mutate(Age=replace_na(Age,110)))$Age\n\n## Curtate future lifetimes for all ages \nfuture_lifetimes <- sapply(ages, \n                           curtate_future_lifetime, \n                           full_df %>% \n                             filter(country==\"U.S.A.\",\n                                    year==2019,\n                                    gender==\"females\") %>% \n                             mutate(Age=replace_na(Age,110)))\n\n## Future lifetime by age \nplot(ages, \n     future_lifetimes, \n     type = 'l', \n     lwd = 2, \n     col = \"green\", \n     xlab = \"Age x\", \n     ylab = \"Future lifetime\", \n     main = \"Future lifetime by age, females, 2019, U.S.A.\")\n## Filter USA. line qx~Age by gender, color by year \nusa = full_df %>% \n         filter(country == \"U.S.A.\")\nggplot(usa, \n       aes(x=Age, y=qx, color = year)) +\ngeom_line(aes(group = year)) + \nfacet_wrap(~gender) +\nlabs(x=\"Age x\", \n     y= expression(paste(\"Mortality rate \", q[x])),\n     title=\"Mortality rates (USA, 1933-2019)\")\n## Filter USA.  \n## Add column life_expectancy \nusa_lifeex = full_df %>% \n         filter(country == \"U.S.A.\") %>% \n         group_by(gender,year) %>% \n         mutate(kpx=cumprod(1-qx),\n                life_expectancy=sum(kpx)) %>% \n         filter(Age==0)\n## line: life_expectancy~year, color by gender \nggplot(usa_lifeex, \n       aes(x=year, y = life_expectancy, color = gender)) +\ngeom_line() + \nlabs(x=\"Year\", \n     y= \"Life Expectancy\",\n     title=\"Life Expectancy, U.S.A.\")"},{"path":"life-expectancy.html","id":"big-data","chapter":"Life Expectancy","heading":"Big Data","text":"","code":""},{"path":"life-expectancy.html","id":"the-general-looks","chapter":"Life Expectancy","heading":"The general looks","text":"","code":"\n## Add column life_expectancy.  \n## Filter Age=0 \nage0_lifeex = full_df %>%\n         group_by(year,country,gender) %>% \n         mutate(Age=replace_na(Age,110),\n                kpx=cumprod(1-qx),\n                life_expectancy=sum(kpx)) %>% \n                filter(Age==0) %>% \n                ungroup()\n## scatter:life_expectancy~year color by country by gender \nggplot(age0_lifeex,\n       aes(x=year,y=life_expectancy,color=country)) + \ngeom_point() +\nfacet_wrap(~gender) +\nggtitle(\"Lifetime in 40 countries between men and women\") +\n  xlab(\"Year\") + ylab(\"Life Expectancy\")\n## Add column life_expectancy \n## Filter Age=0, interesting countries \nsome_age0_lifeex = full_df %>% \n         filter(country %in% c(\"Australia\", \n                               \"Canada\", \n                               \"Hong Kong\", \n                               \"Israel\", \n                               \"Japan\", \n                               \"Netherlands\", \n                               \"New Zealand\", \n                               \"Norway\", \n                               \"Republic of Korea\", \n                               \"Taiwan\", \n                               \"U.K.\", \n                               \"U.S.A.\")) %>% \n         group_by(gender,year) %>% \n         mutate(kpx=cumprod(1-qx),\n                life_expectancy=sum(kpx)) %>% \n         filter(Age==0)\n## life_expectancy~year, color by gender by country \nggplot(some_age0_lifeex, \n       aes(x=year, y = life_expectancy, color = gender)) +\ngeom_line() + \nfacet_wrap(~country) +\nggtitle(\"Comparing women and men lifetime through years between 12 countries\") +\n  xlab(\"Year\") + ylab(\"Life Expectancy\")"},{"path":"life-expectancy.html","id":"big-data-joining-gapminder","chapter":"Life Expectancy","heading":"Big Data joining Gapminder","text":"Turning interests 2007, let’s use available information gapminder conduct analysis:","code":"\ndf_gapminder = full_df %>% \n       inner_join(gapminder,by=c(\"year\",\"country\")) %>% \n       group_by(year,country,gender) %>% \n       mutate(Age=replace_na(Age,110),\n              kpx=cumprod(1-qx),\n              life_expectancy=sum(kpx)) %>% \n       filter(Age==0) %>% \n       ungroup() %>%\n       filter(country %in% c(\"Australia\", \n                               \"Canada\", \n                               \"Hong Kong\", \n                               \"Israel\", \n                               \"Japan\", \n                               \"Netherlands\", \n                               \"New Zealand\", \n                               \"Norway\", \n                               \"Republic of Korea\", \n                               \"Taiwan\", \n                               \"U.K.\", \n                               \"U.S.A.\"))\n\nggplot(df_gapminder,\n       aes(x=year,y=life_expectancy,color=continent)) + \ngeom_point() +\nfacet_wrap(~gender) +\ngeom_text(aes(label=country),hjust=1, vjust=2,size=2) +\nggtitle(\"How lifetime through years behave in continents between men and women\") +\n  xlab(\"Year\") + ylab(\"Life Expectancy\")\n## Scatter:gdpPercap~life_expectancy, 2007 color by continent by gender \ngender_continent_2007 = full_df %>% \n         inner_join(gapminder,by=c(\"year\",\"country\")) %>% \n         filter(country %in% c(\"Australia\", \n                               \"Canada\", \n                               \"Hong Kong\", \n                               \"Israel\", \n                               \"Japan\", \n                               \"Netherlands\", \n                               \"New Zealand\", \n                               \"Norway\", \n                               \"Republic of Korea\", \n                               \"Taiwan\", \n                               \"U.K.\", \n                               \"U.S.A.\"),year==2007) %>%\n         group_by(year,country,gender) %>% \n         mutate(Age=replace_na(Age,110),\n                kpx=cumprod(1-qx),\n                life_expectancy=sum(kpx)) %>% \n         filter(Age==0) %>% \n         ungroup() %>%\n         group_by(year,gender) %>%\n         arrange(desc(life_expectancy))\nggplot(gender_continent_2007,\n       aes(x=gdpPercap,y=life_expectancy,color=continent,size=pop)) + \ngeom_point() +\nfacet_wrap(~gender) +\ngeom_text(aes(label=country),hjust=1, vjust=1,size=2) + \n  ggtitle(\"How lifetime, and GDP per Capital behave between men and women\") +\n  xlab(\"Year\") + ylab(\"Life Expectancy\")\n## Scatter:gdpPercap~life_expectancy, color by continent  \ncontinent_female = full_df %>% \n         inner_join(gapminder,by=c(\"year\",\"country\")) %>% \n         filter(country %in% c(\"Australia\", \n                               \"Canada\", \n                               \"Hong Kong\", \n                               \"Israel\", \n                               \"Japan\", \n                               \"Netherlands\", \n                               \"New Zealand\", \n                               \"Norway\", \n                               \"Republic of Korea\", \n                               \"Taiwan\", \n                               \"U.K.\", \n                               \"U.S.A.\"),gender==\"females\") %>%\n         group_by(year,country,gender) %>% \n         mutate(Age=replace_na(Age,110),\n                kpx=cumprod(1-qx),\n                life_expectancy=sum(kpx)) %>% \n         filter(Age==0) %>% \n         ungroup() %>%\n         group_by(year,gender) %>%\n         arrange(desc(life_expectancy))\n## Females, by years \nggplot(continent_female,\n       aes(x=gdpPercap,y=life_expectancy,color=continent,size=pop)) + \ngeom_point() +\nfacet_wrap(~year) +\ngeom_text(aes(label=country),hjust=1, vjust=1,size=2) +\nggtitle(\"How population, GDP per Capital and lifetime behave through years\") +\n  xlab(\"GDP per Capita\") + ylab(\"Life Expectancy\")"},{"path":"wage-determinants.html","id":"wage-determinants","chapter":"Wage Determinants","heading":"Wage Determinants","text":"\nreport presentation course “Research Methods Finance” students asked use R apply statistical modeling real datasets. scraping careerbuilder.vn, used “Quantile Regression” summarize important information including manager’s average salary.\n","code":""},{"path":"wage-determinants.html","id":"introduction","chapter":"Wage Determinants","heading":"Introduction","text":"Salary one important motivators labor force, existence wage disparity \ninevitable. According economic theory, possible divide causes disparity wages two\ngroups. first group can include elements caused changes labor market, difference \nchange working environment workplace, due different differ nature work due\ndifferences characteristics workers . second group includes causes social\nstigma either caused discrimination society people employment employees. group \ncauses leads inequality society. Therefore, order (1) determine level wage disparity \nVietnam, (2) identify factors actually affect wages (3) decompose wage gap clarify \ndifference.accomplish purposes, writing aims complete following goals:Introduction theoretical basis applicability regression method percentile wage\ndifferential decomposition method based percentile regression.Regression percentile function real wages Vietnam means percentile regression \nbias correction sample selection endogenous remediation.Determine wage gap minimum years experience data collected online\nrecruitment platform.selected research objectives research methods, paper topic scientific powerful\nmeanings:topic applies percentile regression method, regression technique introduced \nKoenker & Bassett (1978) used extensively widespread world yet\npopular Vietnam. Research project Vietnam applying percentile regression\ntechnique, especially applied study wage function decomposition wage difference.financial research project presents concise, complete systematic way theory \npercentile regression.wage function labor groups estimated using Percentile regression method adjustment\nselection bias sample handle endogenous phenomena model, giving estimate solid\nreliable quality.","code":""},{"path":"wage-determinants.html","id":"literature-review","chapter":"Wage Determinants","heading":"Literature Review","text":"nineteen seventy four, Mincer introduced wage equation showing relationship logarithm\nincome factors years schooling, work experience, square experience\nvariable based argument amount wages paid person now depends \nprevious investment human capital. However, equation quite tough estimate since involves \nlogarithm lot variables contribute complicated model.Mincer earnings function:\\[\\ln(\\omega) = \\ln(\\omega_0)+\\rho s+ \\beta_1x+\\beta_2x^2\\]\\(\\omega\\): earnings\\(\\omega_0\\): earnings someone education experience\\(s\\): years education\\(x\\): years experience\\(\\rho, \\beta_1, \\beta_2\\): regression coefficientsAfter Card D’s research (1994), many studies extended Mincer’s wage equation using different\nindependent variables open Mincer (1974) wage function.\ntypical studies percentile regression applied:Salary analysis: Starting study Edgewort (1922). Followed studies Becker\n(1957), Dunlop (1957), Slichter (1950), Cullen (1956), Dalton & Ford (1977) Long &\nLink(1983), Dickens & Katz (1987), Krueger & Summers(1988) Groshen (1991), Ferber &\nGreen (1982); Lindley, Fish Jackson (1992), Blackaby, Booth Frank (2005).typical studies wage disparity applied past: reduce percentiles wage function:\nBuchinsky (1994) initiated application percentile regression method estimating function\nsalary variable regression. Followed Fortin studies Lemieux (1998), Ajwad et al.\n(2002), Albrecht et al associates (2003), Machado & Mata (2005), Melly (2006)\nGunawardena(2006) Arulampalam et al (2007), Nestic (2010), Del Río, Gradín&Cantó (2011).studies Vietnam use percentile regression regression wage function wage differential\ndecomposition. Typical can mentioned study Hung et al (2007a) Hung T.P \nassociates (2007b).","code":""},{"path":"wage-determinants.html","id":"data","chapter":"Wage Determinants","heading":"Data","text":"data collected recruitment website “careerbuilder.vn” filtering jobs Ho Chi Minh city finance majoring Banking, Insurance, Real Estate Security.Specifically, specific recruitment, shall corresponding information including salary,\nexperience, education, . Frequently, salary often hidden found ranging format interest paper cares minimum years experience ranging 8 million 50 million.Looking data frame, clear see 3 categorical variables education, areas levels.\nalso obvious age, experience, salary considered continuous variables.turn categorical columns dummy variables numeric variables represents\ncategorical data. practical matter, regression results easiest interpret dummy variables \nlimited two specific values, 0 1. Typically, 1 represents presence qualitative attribute, 0\nrepresents absence.total, 11 binary variables 3 continuous variables since investigate relationship\nminimum values continuous variables including minimum age, minimum experience,\nminimum salary.","code":"\nmydata <- read.csv(\"https://raw.githubusercontent.com/ThanhDatIU/RMF_IU/main/careerbuilder.csv\")"},{"path":"wage-determinants.html","id":"methodology","chapter":"Wage Determinants","heading":"Methodology","text":"","code":""},{"path":"wage-determinants.html","id":"linear-regression-model","chapter":"Wage Determinants","heading":"Linear Regression Model","text":"Model equation:\n\\[y = \\beta X\\]Mean squared error linear regression\n\\[MSE = \\frac{1}{n} \\sum_{= 1}^{n}(y_i-\\beta x_i)^2\\]Minimization problem:\n\\[\\hat\\beta = \\text{argmin}_\\beta \\frac{1}{n} \\sum_{= 1}^{n} (y_i-\\beta x_i)^2\\]best linear regression model \\(Y\\) equals beta \\(X\\) estimated establishing mean squared error measuring average squares errors.\ncoefficients OLS found coming beta hat mean squared error minimized.Similarly, quantile regression model estimated establishing mean squared error measuring average absolute deviations central point.\ncoefficients quantile regression found generating betas hat mean absolute deviation minimized.","code":""},{"path":"wage-determinants.html","id":"quantile-regression-model","chapter":"Wage Determinants","heading":"Quantile regression model","text":"percentile regression method introduced Koenker& Bassett first time 1978.Model equation:\n\\[y = \\beta X\\]Mean squared error linear regression\\[MAD = \\sum_{:y_i > \\beta x_i}^{n}\\tau |y_i-\\beta x_i| + \\sum_{:y_i < \\beta x_i}^{n}(1-\\tau) |y_i-\\beta x_i|\\]Minimization problem:\\[\\hat\\beta = \\text{argmin}_\\beta \\frac{1}{n} \\left(\\sum_{:y_i > \\beta x_i}^{n}\\tau |y_i-\\beta x_i| + \\sum_{:y_i < \\beta x_i}^{n}(1-\\tau) |y_i-\\beta x_i| \\right)\\]\n\\(\\tau\\) one half, MAD symmetrical median always number data points .\ninstead absolute residuals weighted differently depending whether positive negative, can calculate quantiles distribution.\nestimate tau quantile, set weight positive observations tau negative observations \\(1 – \\tau\\).\ncan select quantiles interest common choices \\(0.1, 0.5, 0.9\\).","code":""},{"path":"wage-determinants.html","id":"results","chapter":"Wage Determinants","heading":"Results","text":"","code":""},{"path":"wage-determinants.html","id":"visualization","chapter":"Wage Determinants","heading":"Visualization","text":"","code":""},{"path":"wage-determinants.html","id":"salary-age","chapter":"Wage Determinants","heading":"Salary ~ Age","text":"many companies recruit applicant degree studies intermediate school, number jobs requiring university college quite high.\ncloser look university facet suggests correlation coefficients minimum age minimum salary considerably low implying linear regression may working relationship.","code":""},{"path":"wage-determinants.html","id":"salary-exp","chapter":"Wage Determinants","heading":"Salary ~ Exp","text":"hand, comes minimum years experience x-axis, obvious say strong linear relationship minimum years experience minimum salary.\nHowever, constant variance assumption may violated given fact data points 3 minimum years experience vary much noticeably one year less working experience.","code":""},{"path":"wage-determinants.html","id":"linear-regression-model-1","chapter":"Wage Determinants","heading":"Linear Regression Model","text":"","code":""},{"path":"wage-determinants.html","id":"multiple-linear-regression-model","chapter":"Wage Determinants","heading":"Multiple Linear Regression Model","text":"Using linear model function familiar stats library, ’s clear see education variables significant.\nInsurance meaningful implying areas finance maybe explain model.\nmatrix singularity comes variable level may drop one factor later analysis.\nimportant thing take notice linear model two continuous variables age experience. previously acknowledged, age statistically significant, minimum experience variable explains model corresponding two-point six.Education significant.Insurance salary significant*.Levels significant*.Minimum age significant.Minimum experience significant*.","code":"#> \n#> Call:\n#> lm(formula = min_salary ~ university + college + intermediate + \n#>     education_not_required + banking + insurance + security + \n#>     real_estate + employee + observer + manager + min_age + min_exp, \n#>     data = df_nogender_agelimited)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -9.3386 -3.8286 -0.6764  2.4357 29.9892 \n#> \n#> Coefficients: (2 not defined because of singularities)\n#>                        Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)             13.1401     8.6069   1.527  0.12947    \n#> university              -0.2435     2.6976  -0.090  0.92823    \n#> college                 -2.1271     2.7371  -0.777  0.43861    \n#> intermediate            -3.3352     2.9914  -1.115  0.26712    \n#> education_not_required       NA         NA      NA       NA    \n#> banking                  3.0140     3.1842   0.947  0.34576    \n#> insurance                5.4813     2.0157   2.719  0.00751 ** \n#> security                 3.1239     2.4575   1.271  0.20613    \n#> real_estate              3.7259     3.4863   1.069  0.28733    \n#> employee                -9.5280     1.7146  -5.557 1.68e-07 ***\n#> observer                -6.4152     2.0629  -3.110  0.00234 ** \n#> manager                      NA         NA      NA       NA    \n#> min_age                  0.1229     0.3052   0.403  0.68780    \n#> min_exp                  2.6574     0.4484   5.927 3.04e-08 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 6.349 on 120 degrees of freedom\n#> Multiple R-squared:  0.5527, Adjusted R-squared:  0.5117 \n#> F-statistic: 13.48 on 11 and 120 DF,  p-value: < 2.2e-16"},{"path":"wage-determinants.html","id":"simple-linear-regression","chapter":"Wage Determinants","heading":"Simple Linear Regression","text":"","code":""},{"path":"wage-determinants.html","id":"areas","chapter":"Wage Determinants","heading":"Areas","text":"let’s try use linear model areas finance variable.\n’s interesting can generally summarize average salary majors finance insurance standing highest average salary 26 million VND. majors seem consistent range 15 18 million per month.Banking salary 15 million VND average.Insurance salary 26 million VND average.Real estate salary 18 million VND average.Security salary 16 million VND average.","code":"#> \n#> Call:\n#> lm(formula = min_salary ~ areas + 0, data = df_nogender_agelimited)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -11.750  -5.889  -3.231   2.383  36.769 \n#> \n#> Coefficients:\n#>                  Estimate Std. Error t value Pr(>|t|)    \n#> areasbanking       15.617      1.218  12.826  < 2e-16 ***\n#> areasinsurance     26.750      4.474   5.979 2.09e-08 ***\n#> areasreal_estate   18.231      1.110  16.427  < 2e-16 ***\n#> areassecurity      16.889      2.982   5.663 9.34e-08 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 8.947 on 128 degrees of freedom\n#> Multiple R-squared:  0.7969, Adjusted R-squared:  0.7905 \n#> F-statistic: 125.5 on 4 and 128 DF,  p-value: < 2.2e-16\nggplot(df_nogender_agelimited, aes(x = as.factor(areas), y = min_salary, color = education)) + \n  geom_point() +\n  theme_bw()"},{"path":"wage-determinants.html","id":"levels","chapter":"Wage Determinants","heading":"Levels","text":"Chances manager earn minimum salary 26 million considerably higher number observers employees 14 18 million respectively.Manager salary 26 million VND average.Observer salary 18 million VND average.Employee salary 14 million VND average.","code":"#> \n#> Call:\n#> lm(formula = min_salary ~ levels + 0, data = df_nogender_agelimited)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -15.125  -4.473  -2.049   3.375  40.527 \n#> \n#> Coefficients:\n#>                Estimate Std. Error t value Pr(>|t|)    \n#> levelsemployee  14.4730     0.8329   17.38   <2e-16 ***\n#> levelsmanager   26.6250     1.6039   16.60   <2e-16 ***\n#> levelsobserver  18.9579     1.8027   10.52   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 7.858 on 129 degrees of freedom\n#> Multiple R-squared:  0.8421, Adjusted R-squared:  0.8385 \n#> F-statistic: 229.4 on 3 and 129 DF,  p-value: < 2.2e-16\nggplot(df_nogender_agelimited, \n       aes(x = factor(levels, levels = c(\"employee\", \"observer\", \"manager\")), \n           y = min_salary, color = education)) + \n  geom_point() +\n  theme_bw()"},{"path":"wage-determinants.html","id":"experience-1","chapter":"Wage Determinants","heading":"Experience","text":"Experience significant, 1 year experience average salary increase 3 million VND.lastly, continuous variable experience despite highly significant estimate, model working well multiple R squared 30 percent. ’s need Quantile Regression.","code":"#> \n#> Call:\n#> lm(formula = min_salary ~ min_exp, data = df_nogender_agelimited)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -10.520  -5.010  -1.937   2.772  37.772 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  10.6453     1.0983   9.693  < 2e-16 ***\n#> min_exp       3.2915     0.4323   7.614 4.81e-12 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 7.585 on 130 degrees of freedom\n#> Multiple R-squared:  0.3084, Adjusted R-squared:  0.3031 \n#> F-statistic: 57.97 on 1 and 130 DF,  p-value: 4.812e-12\nggplot(df_nogender_agelimited, \n       aes(x = as.numeric(min_exp), \n           y = min_salary, \n           color = education)) + \n  geom_point()"},{"path":"wage-determinants.html","id":"quantile","chapter":"Wage Determinants","heading":"Quantile","text":"","code":""},{"path":"wage-determinants.html","id":"table","chapter":"Wage Determinants","heading":"Table","text":"Drop observer avoiding matrix singularity.Insurance predicted using OLS.Employee predicted using OLS.Manager predicted using OLS.Observer predicted using OLS.Minimum experience predicted using OLS salary less 55% greaterr 80% less 31$ million VND greater 41.5$ million VND*.Regressing Salary Minimum Years Experience variable, distinct disparity across ranges seems observable. next step employing visualizing tools illustrate coefficients vary across percentiles.","code":""},{"path":"wage-determinants.html","id":"graph","chapter":"Wage Determinants","heading":"Graph","text":"","code":""},{"path":"wage-determinants.html","id":"salary-min_age","chapter":"Wage Determinants","heading":"Salary ~ min_age","text":"terms min_age, coefficients significant lower 45 percentile, coefficients significantly different confidence internals OLS minimum reaching 1 5 percent lower tail. opposite true upper 80 percentile maximum coefficient stood exactly 5 considering 5 percent upper tail.","code":""},{"path":"wage-determinants.html","id":"salary-min_exp","chapter":"Wage Determinants","heading":"Salary ~ min_exp","text":"scatter plot compares performance Linear Model Quantile Model. obvious suggest Quantile represents natural flexible way capture complexities inherent relationship estimating models conditional quantile functions.","code":""},{"path":"wage-determinants.html","id":"conclusions","chapter":"Wage Determinants","heading":"Conclusions","text":"percentile regression approach, first developed Koenker Bassett (1978) widely used throughout world yet popular Vietnam, employed project. research projects Vietnam use percentile regression approach, particularly useful studying pay functions decomposing wage differences. financial report explains percentile regression theory clear, comprehensive, systematic manner. pay function labor groups estimated using percentile regression approach selection bias sample correction endogenous phenomena handled model, yielding solid trustworthy estimate.experience gap across pay distribution examined depth previous sections using data Careerbuilder.vn quantile impact salaries less 27 million 40 million, according aggregate results entire sample. reason might finance major may need two years work experience order employee earn reasonable wage.accomplishments, code snippet acknowledged Appendix considered typical source learning Regression model RStudio.","code":""},{"path":"wage-determinants.html","id":"references","chapter":"Wage Determinants","heading":"References","text":"Koenker, R. Bassette, G., 1978. Regression quantiles. [online] Available : https://www.jstor.org/stable/1913643Tran, T. . T. (2018, February). Investigating gender wage gap Vietnam quantile regression: Sticky floor glass ceiling. [online] Available : https://www.researchgate.net/publication/323870909_Investigating_the_gender_wage_gap_in_Vietnam_by_ quantile_regression_Sticky_floor_or_glass_ceilingTran, T. . T. (2014, June). Ước lượng hàm hồi quy tiền lương ở Việt Nam giai đoạn 2002 - 2010 bằng thủ tục Heckman hai bước. [online] Available : http://jabes.ueh.edu.vn/Home/SearchArticle?article_Id = 2e855945-ac00-487d-a4f7-8bcec142927f","code":""},{"path":"wage-determinants.html","id":"appendix","chapter":"Wage Determinants","heading":"Appendix","text":"","code":"\n## ----SETUP, include = FALSE-----------------------------------------------------------------------------------\nlibrary(plyr)\nknitr::opts_chunk$set(echo = TRUE)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(readxl)\nlibrary(highcharter)\nlibrary(tidyquant)\nlibrary(timetk)\nlibrary(tibbletime)\nlibrary(quantmod)\nlibrary(PerformanceAnalytics)\nlibrary(scales)\nlibrary(plotly)\nlibrary(rvest)\nlibrary(xml2)\nlibrary(tibble)\nlibrary(quantreg)\nlibrary(SparseM)\nlibrary(tidyverse)\nlibrary(kableExtra)\nlibrary(rmarkdown)\n\n\n## -------------------------------------------------------------------------------------------------------------\nmydata <- read.csv(\"https://raw.githubusercontent.com/ThanhDatIU/RMF_IU/main/careerbuilder.csv\")\n\n## ---- message = FALSE, warning = FALSE, echo = FALSE----------------------------------------------------------\ntable = mydata\npaged_table(table)\n\n\n## ---- message = FALSE, warning = FALSE, echo = FALSE----------------------------------------------------------\ndf = mydata %>% mutate(\n  education = ifelse(university == 1, \"university\", \n            ifelse(college == 1, \"college\", \n            ifelse(intermediate == 1, \"intermediate\", \"education_not_required\"))), \n  areas = ifelse(banking == 1, \"banking\", \n        ifelse(insurance == 1, \"insurance\", \n        ifelse(security == 1, \"security\", \"real_estate\"))), \n  levels = ifelse(employee == 1, \"employee\", \n         ifelse(observer == 1, \"observer\", ifelse(manager == 1, \"manager\", \"chief_manager\"))), \n  gender = ifelse(male == 1, 1, ifelse(female == 1, 1, 0)))\n\ndf_nogender_agelimited = df %>% \n  filter(age != \"not_limited\", levels != \"chief_manager\", gender == 0) %>% \n  mutate(min_age = as.numeric(min_age))\n\n## ---- message = FALSE, warning = FALSE, echo = FALSE----------------------------------------------------------\ntable = df_nogender_agelimited %>% select(education, \n                                          areas, \n                                          levels, \n                                          min_age, \n                                          max_age, \n                                          min_exp, \n                                          max_exp, \n                                          min_salary, \n                                          max_salary)\npaged_table(table)\n\n\n## ---- message = FALSE, warning = FALSE, echo = FALSE----------------------------------------------------------\nggplot(df_nogender_agelimited, \n       aes(x = as.numeric(min_age), \n           y = min_salary, \n           color = areas, \n           shape = levels)) + \n  geom_point() + \n  facet_wrap(~education) + \n  labs(x = \"Minimum Age\", \n       y = \"Minimum Salary\", \n       title = \"CareerBuilder Ho Chi Minh City Jobs in 2021\") +\n  scale_x_continuous(limits = c(18, 36))\n\n\n## ---- message = FALSE, warning = FALSE, echo = FALSE----------------------------------------------------------\nggplot(df_nogender_agelimited, aes(x = min_exp, \n                                   y = min_salary, \n                                   color = areas, \n                                   shape = levels)) + \n  geom_point() + \n  facet_wrap(~education) +\n  labs(title = \"CareerBuilder Ho Chi Minh City Jobs in 2021\", \n        x = \"Minimum Age\", y = \"Minimum Salary\") + \n  theme_bw() +\n  scale_x_continuous(limits = c(0, 10), n.breaks = 10)\n\n\n## ---- message = FALSE, warning = FALSE, echo = FALSE----------------------------------------------------------\nmale = df_nogender_agelimited$male\nfemale = df_nogender_agelimited$female\nuniversity = df_nogender_agelimited$university\ncollege = df_nogender_agelimited$college\nintermediate = df_nogender_agelimited$intermediate\neducation_not_required = df_nogender_agelimited$education_not_required\nbanking = df_nogender_agelimited$banking\ninsurance = df_nogender_agelimited$insurance\nsecurity = df_nogender_agelimited$security\nreal_estate = df_nogender_agelimited$real_estate\nemployee = df_nogender_agelimited$employee\nobserver = df_nogender_agelimited$observer\nmanager = df_nogender_agelimited$manager\nchief_manager = df_nogender_agelimited$chief_manager\nage = df_nogender_agelimited$age\nmin_age = df_nogender_agelimited$min_age\nmax_age = df_nogender_agelimited$max_age\nmin_exp = df_nogender_agelimited$min_exp\nmax_exp = df_nogender_agelimited$max_exp\nmin_salary = df_nogender_agelimited$min_salary\nmax_salary = df_nogender_agelimited$max_salary\neducation = df_nogender_agelimited$education\nareas = df_nogender_agelimited$areas\nlevels = df_nogender_agelimited$levels\ngender = df_nogender_agelimited$gender\n\n# OLS regression\nolsreg <- lm(min_salary~university+\n                        college+\n                        intermediate+\n                        education_not_required+\n                        banking+\n                        insurance+\n                        security+\n                        real_estate+\n                        employee+\n                        observer+\n                        manager+\n                        min_age+\n                        min_exp, \n             data = df_nogender_agelimited)\nsummary(olsreg)\n\n\n## ---- message = FALSE, warning = FALSE, echo = FALSE----------------------------------------------------------\n# OLS regression\nolsreg <- lm(min_salary ~ areas+0, data = df_nogender_agelimited)\nsummary(olsreg)\n\n\n## -------------------------------------------------------------------------------------------------------------\nggplot(df_nogender_agelimited, aes(x = as.factor(areas), y = min_salary, color = education)) + \n  geom_point() +\n  theme_bw()\n\n\n## ---- message = FALSE, warning = FALSE, echo = FALSE----------------------------------------------------------\n# OLS regression\nolsreg <- lm(min_salary ~ levels+0, data = df_nogender_agelimited)\nsummary(olsreg)\n\n\n## -------------------------------------------------------------------------------------------------------------\nggplot(df_nogender_agelimited, \n       aes(x = factor(levels, levels = c(\"employee\", \"observer\", \"manager\")), \n           y = min_salary, color = education)) + \n  geom_point() +\n  theme_bw()\n\n\n## ---- message = FALSE, warning = FALSE, echo = FALSE----------------------------------------------------------\n# OLS regression\nolsreg <- lm(min_salary ~ min_exp, \n             data = df_nogender_agelimited)\nsummary(olsreg)\n\n\n## -------------------------------------------------------------------------------------------------------------\nggplot(df_nogender_agelimited, \n       aes(x = as.numeric(min_exp), \n           y = min_salary, \n           color = education)) + \n  geom_point()\n\n\n## ---- message = FALSE, warning = FALSE, echo = FALSE----------------------------------------------------------\nquantreg.all <- rq(min_salary~banking+\n                     insurance+\n                     security+\n                     real_estate+\n                     employee+\n                     manager+\n                     min_exp, tau = seq(0.05, 0.95, by = 0.05),                   \n                   data = df_nogender_agelimited)\nquantreg.plot <- summary(quantreg.all)\nplot(quantreg.plot, xlim = c(0, 1), ylim = c(-6, 11))\n\n\n## ---- message = FALSE, warning = FALSE, echo = FALSE----------------------------------------------------------\nquantreg.all <- rq(min_salary~min_exp, \n                   tau = seq(0.05, 0.95, by = 0.05), \n                   data = mydata)\nquantreg.plot <- summary(quantreg.all)\nplot(quantreg.plot, xlim = c(0, 1), ylim = c(2, 4.5))\n\n\n## ---- message = FALSE, warning = FALSE, echo = FALSE----------------------------------------------------------\nlibrary(broom)\nquantreg.all <- rq(min_salary~min_exp, \n                   tau = seq(0.05, 0.95, by = 0.05), \n                   data = mydata)\n\nquantreg.tidied <- tidy(quantreg.all)\n\nlibrary(tidyr)\nquantreg.wide = quantreg.tidied %>% \n  select(term, tau, estimate) %>% \n  spread(tau, estimate)\n\n## ---- message = FALSE, warning = FALSE, echo = FALSE----------------------------------------------------------\npaged_table(quantreg.wide)\n\n\n## ---- message = FALSE, warning = FALSE, echo = FALSE----------------------------------------------------------\nggplot(df_nogender_agelimited, aes(x = as.numeric(min_age), y = min_salary)) + \n  geom_point() + \n  geom_smooth(method = \"lm\") + \n  geom_quantile(quantiles = c(1:9/10), color = \"red\") +\n  scale_x_continuous(limits = c(18, 36))\n\n\n## ---- message = FALSE, warning = FALSE, echo = FALSE----------------------------------------------------------\n#### Salary ~ Exp\nggplot(df_nogender_agelimited, aes(x = min_exp, y = min_salary)) + \n  geom_point() + \n  geom_smooth(method = \"lm\") + \n  geom_quantile(quantiles = c(1:9/10), color = \"red\") +\n  scale_x_continuous(limits = c(0, 10), n.breaks = 10)"},{"path":"ielts-reading-topics.html","id":"ielts-reading-topics","chapter":"IELTS Reading Topics","heading":"IELTS Reading Topics","text":"\nproject attempts model underlying topics reading passages Cambridge English IELTS 11 15. begin , raw text data scraped https://study4.com/tests using package rvest. packages tidytext well tidyverse implemented turn raw data tidy data frame acknowledge relevant statistics corresponding reading passages. two final parts demonstrate work flow text mining topic modeling 2 packages quanteda & stm conclude interested information.\n","code":""},{"path":"ielts-reading-topics.html","id":"scrape-data-with-rvest","chapter":"IELTS Reading Topics","heading":"Scrape data with rvest","text":"\nR, ’s easy read HTML using rvest read_html() function. hierarchic nature HTML scraping, ’s always root node. root branches lead nodes, language HTML, children. rvest, ’s easy quickly traverse tree select nodes ’re interested .\n\nfunction ’re probably going use working rvest html_nodes(). takes HTML document -called node set input returns nodes, specifically, xml_nodesets. html_node() without plural special case returns first node matches selection.\n\nhtml_nodes() take html document, also -called selector. string specifies path html tree. selector also adheres specific syntax, , descendant syntax. select text headers, write “//” followed “h2”.\n\ncan also extract text html_text() function href attribute link html_attrs(). plural form html_attrs() returns attributes element named vector.\n","code":""},{"path":"ielts-reading-topics.html","id":"all-test-available-in-4study","chapter":"IELTS Reading Topics","heading":"All test available in 4study","text":"\nVisit https://study4.com/tests, search “IELTS cam reading” store link page.\n\nUse read_html, html_nodes, html_attr find element containing link test store links links.\n\nConcatenate homepage domain sub-domain links.\n","code":"\nlibrary(rvest)\n\n## Filter interested tests from the website and store the link to var page \npage=\"https://study4.com/tests/?term=IELTS+cam+reading&page=1\"\n## Initiate var links \nlinks= NULL\n  \n## Read in links as HTML document, navigate to needed HTML elements attribute and extract those hyperlink sub-domain \nlinks=read_html(page) %>% \n    html_nodes(xpath=\"/html/body/div[3]/div[2]/div[3]/div/div[1]/div[1]/div/div/div/a\") %>% \n    html_attr('href')\nhead(links)#> [1] \"/tests/1026/cam-ielts-10-reading-test-1/\"\n#> [2] \"/tests/31/cam-ielts-10-reading-test-2/\"  \n#> [3] \"/tests/1027/cam-ielts-10-reading-test-3/\"\n#> [4] \"/tests/33/cam-ielts-10-reading-test-4/\"  \n#> [5] \"/tests/26/cam-ielts-11-reading-test-1/\"  \n#> [6] \"/tests/27/cam-ielts-11-reading-test-2/\"\n## Paste the homepage link to those sub-domain \nlinks_full=paste(\"https://study4.com\",links,\"start\",sep=\"\")\n\n## Investigate the first 6 out of 60 links \nhead(links_full)#> [1] \"https://study4.com/tests/1026/cam-ielts-10-reading-test-1/start\"\n#> [2] \"https://study4.com/tests/31/cam-ielts-10-reading-test-2/start\"  \n#> [3] \"https://study4.com/tests/1027/cam-ielts-10-reading-test-3/start\"\n#> [4] \"https://study4.com/tests/33/cam-ielts-10-reading-test-4/start\"  \n#> [5] \"https://study4.com/tests/26/cam-ielts-11-reading-test-1/start\"  \n#> [6] \"https://study4.com/tests/27/cam-ielts-11-reading-test-2/start\""},{"path":"ielts-reading-topics.html","id":"scrape-cambridge-10-to-15","chapter":"IELTS Reading Topics","heading":"Scrape Cambridge 10 to 15","text":"\nlinks_full, filtering test 11 15.\n\nNavigate left screen selecting class ‘question-twocols-left’ collectively descending 3 div children get elements containing paragraphs. links_scrape yield 3 results. Store results nodes shall list containing 60 nodes\n","code":"\n## Only scraping Cambridge 10 to 15 \nlinks_scrape=links_full[5:24]\n## Initiate nodes \nnodes=NULL\n\n## Read in each element of links_scrape as HTML document, navigate to needed HTML elements attribute and extract html element div containing needed paragraphs then cumulatively store them to nodes\nfor(i in 1:length(links_scrape)){\n  reading=read_html(links_scrape[i]) %>%\n    html_nodes(xpath=\"//div[@class='question-twocols-left']/div/div/div\")\n  nodes=c(nodes,reading)\n}\n\n## Investigate the first 6 out of 60 nodes \nhead(nodes)#> [[1]]\n#> {html_node}\n#> <div>\n#> [1] <div>\\n<p><span>You should spend about 20 minutes on <\/span><strong>Quest ...\n#> \n#> [[2]]\n#> {html_node}\n#> <div>\n#> [1] <div>\\n<p><span>You should spend about 20 minutes on <strong>Questions 14 ...\n#> \n#> [[3]]\n#> {html_node}\n#> <div>\n#> [1] <div>\\n<p><span>You should spend about 20 minutes on <strong>Questions 27 ...\n#> \n#> [[4]]\n#> {html_node}\n#> <div>\n#> [1] <div>\\n<p><span>You should spend about 20 minutes on <\/span><strong>Quest ...\n#> \n#> [[5]]\n#> {html_node}\n#> <div>\n#> [1] <div>\\n<p><span>You should spend about 20 minutes on <strong>Questions 14 ...\n#> \n#> [[6]]\n#> {html_node}\n#> <div>\n#> [1] <div>\\r\\n<p>You should spend about 20 minutes on <strong>Questions 27-40< ..."},{"path":"ielts-reading-topics.html","id":"passages-to-a-data-frame","chapter":"IELTS Reading Topics","heading":"Passages to a data frame","text":"\nSince elements nodes 2 tag children h2 p, write 2 functions extract heading reading tests respectively.\n\nClean header Preface ‘half thinks:\\nAdventures mathematical reasoning’ construct data frame composite primary key book, test passage\n\n","code":"\n## Write 2 functions to extract reading and header from those div element \n\n## Use as.character() to transform those div nodes into html document, read them in and filter only paragraph elements and notice how the first paragraph element p[1], the  reading instructions, is ignored. Turn these p to text and concatenate with the separator new line characters\nnodes_to_reading=function(i){\n  as.character(nodes[[i]]) %>% \n    read_html() %>% \n    html_nodes(xpath='//p') %>% \n    .[-1] %>% \n    html_text(trim=TRUE) %>% \n    paste(collapse=\"\\n\")\n}\n\n## Use as.character() to transform those div nodes into html document, read them in and filter only html element h2 containing the passage titles. Turn these p to text and concatenate with the separator new line characters\nnodes_to_header=function(i){\n  as.character(nodes[[i]]) %>% \n    read_html() %>% \n    html_nodes(xpath='//h2') %>% \n    html_text(trim=TRUE) %>% \n    paste(collapse=\"\\n\")\n}\n\n## Apply 2 functions to obtain 2 vectors containing 60 elements \nreading=sapply(1:60,nodes_to_reading)\nheader=sapply(1:60,nodes_to_header)\n## Clean header: Preface to ‘How the other half thinks:\\nAdventures in mathematical reasoning’ \nheader=str_replace_all(header, \"\\n\",\" \")\n\n## Put them all to get a data frame \ndf=data.frame(book=as.factor(rep(11:15, each=12)),\n              test=as.factor(rep(rep(1:4,each=3),5)),\n              passage=as.factor(rep(seq(1:3),20)),\n              header=header,\n              reading=reading)"},{"path":"ielts-reading-topics.html","id":"statistics-with-tidyverse-tidytext","chapter":"IELTS Reading Topics","heading":"Statistics with tidyverse & tidytext","text":"process splitting text referred tokenization. data frame named df let’s call unnest_tokens() performs tokenization package tidytext. tutorial focusing text words, general tokens can sequence characters, sequence words let’s just look.","code":""},{"path":"ielts-reading-topics.html","id":"sentences","chapter":"IELTS Reading Topics","heading":"Sentences","text":"function takes column table, splits sentences , default, drop column original text. get back table sentence row counted using function count() package dplyr, already included tidyverse, count numbers sentences 26 57.","code":"\nlibrary(tidyverse)\nlibrary(tidytext)\n\nsentences=df %>% \n  unnest_tokens(sentences,reading,token=\"sentences\")\n\ncount_sentences=sentences %>% \n  group_by(book,test,passage,header) %>% \n  count(header,sort=TRUE)\nkable(head(count_sentences,10))"},{"path":"ielts-reading-topics.html","id":"words","chapter":"IELTS Reading Topics","heading":"Words","text":"Performing similar behaviors words, shall get count ranges 750 950. examination suggest numbers words passage one two three rather consistent standing 854, 861 878 words order.’s , let’s graph 5 books using package ggplot2, already included tidyverse, demonstrate words vary across books.","code":"\ntidy_df=df %>% \n  unnest_tokens(word,reading,token=\"words\")\n\ncount_words=tidy_df %>%  \n  group_by(book,test,passage,header) %>% count(header,sort=TRUE)\nkable(head(count_words,10))\n## Average words in a passage \naverage=count_words %>% \n  group_by(passage) %>% \n  summarize(mean(n))\naverage#> # A tibble: 3 × 2\n#>   passage `mean(n)`\n#>   <fct>       <dbl>\n#> 1 1            855.\n#> 2 2            861.\n#> 3 3            879.\n## Plot words~book \n ggplot(count_words,\n        aes(x=book,\n            y=n,\n            size=passage,\n            color=test)) + \n  geom_point()\n## Book with lowest words \nlowest_book=count_words %>% \n  group_by(book) %>% \n  summarize(total_words=sum(n)) %>% \n  arrange(total_words)\n\nlowest_book#> # A tibble: 5 × 2\n#>   book  total_words\n#>   <fct>       <int>\n#> 1 12          10015\n#> 2 11          10271\n#> 3 14          10466\n#> 4 15          10525\n#> 5 13          10627\n## Test with lowest words \nlowest_test=count_words %>% \n  group_by(book,test) %>% \n  summarize(total_words=sum(n)) %>% \n  arrange(total_words)\n\nlowest_test#> # A tibble: 20 × 3\n#> # Groups:   book [5]\n#>    book  test  total_words\n#>    <fct> <fct>       <int>\n#>  1 12    1            2285\n#>  2 15    4            2465\n#>  3 12    3            2473\n#>  4 14    4            2474\n#>  5 11    3            2505\n#>  6 11    1            2529\n#>  7 11    4            2559\n#>  8 15    1            2592\n#>  9 12    4            2597\n#> 10 14    2            2609\n#> 11 13    2            2617\n#> 12 13    3            2634\n#> 13 13    1            2638\n#> 14 12    2            2660\n#> 15 15    3            2672\n#> 16 14    3            2674\n#> 17 11    2            2678\n#> 18 14    1            2709\n#> 19 13    4            2738\n#> 20 15    2            2796"},{"path":"ielts-reading-topics.html","id":"clean-documents-with-tidytext-qdap","chapter":"IELTS Reading Topics","heading":"Clean documents with tidytext & qdap","text":"","code":""},{"path":"ielts-reading-topics.html","id":"top-frequent-words","chapter":"IELTS Reading Topics","heading":"Top frequent words","text":"clean documents? First, re-visit function count() obtain frequencies words within whole corpus. counts, often want examine frequent words. can done sorting rows within group order descending counts, realizing rank word equal row number. frequent word row 1, second frequent - row 2, . However, result table lot common English stopwords.words considered noise text necessarily removed. good example indefinite definite articles “” “”. Stopwords often obscure word associations topics: frequent words show top frequency tables, pushing important words sight. previous lesson example showing five probable words two topics. Words “”, “”, “” output contribute understanding topics .","code":"\n## Top frequent words \nfrequent_count=tidy_df %>%  \n  count(word, sort = TRUE) %>%\n  top_n(10)\npaged_table(frequent_count)"},{"path":"ielts-reading-topics.html","id":"remove-stopwords","chapter":"IELTS Reading Topics","heading":"Remove stopwords","text":"Let’s say familiar inner join two tables, merging tables using column presenting key tables indicating rows match. rows matching key values make output. Opposite , anti_join() drops rows matching key values. perfect situations remove rows based criterion. perform anti_join(), stopwords removed.However, might make sense words “studies” singular form “study” considered one term.","code":"\nlibrary(tidytext)\n\nnostopwords_df=tidy_df %>% \n  anti_join(stop_words)\n\nnostopwords_df_count=nostopwords_df %>%  \n  count(word, sort = TRUE) %>%\n  top_n(100)\npaged_table(nostopwords_df_count)"},{"path":"ielts-reading-topics.html","id":"clean-words","chapter":"IELTS Reading Topics","heading":"Clean words","text":"\nbag words text mining, cleaning helps aggregate terms. Specific preprocessing steps vary based project. example, words used IELTS reading vastly different used tweets, cleaning process can also quite different.\n\nCommon preprocessing functions include:\n\ntolower(): Make characters lowercase\n\nremovePunctuation(): Remove punctuation marks\n\nremoveNumbers(): Remove numbers\n\nstripWhitespace(): Remove excess whitespace\n\ntolower() part base R, three functions come tm package.\n\nqdap package offers text cleaning functions. useful way particularly powerful combined others.\n\nbracketX(): Remove text within brackets (e.g. “’s () cool” becomes “’s cool”)\n\nreplace_number(): Replace numbers word equivalents (e.g. “2” becomes “two”)\n\nreplace_abbreviation(): Replace abbreviations full text equivalents (e.g. “Sr” becomes “Senior”)\n\nreplace_contraction(): Convert contractions back base words (e.g. “shouldn’t” becomes “”)\n\nreplace_symbol() Replace common symbols word equivalents (e.g. “$” becomes “dollar”)\n\nStill, another useful preprocessing step involves word-stemming stem completion. Word stemming reduces words unify across documents. example, stem “computational”, “computers” “computation” “comput”.\n\ntm package provides stemDocument() function get word’s root. function either takes character vector returns character vector, takes PlainTextDocument returns PlainTextDocument.\n","code":"\nlibrary(qdap)\n\nclean_documents= function(word){\n  word=str_replace_all(word, \"’\",\"'\")\n  word=replace_contraction(word)\n  word=replace_number(word)\n  word=replace_abbreviation(word)\n  word=replace_symbol(word)\n  word=stemDocument(word)\n  word=tolower(word)\n  return(word)\n}\n\nclean_df=nostopwords_df %>% \n  mutate(word=clean_documents(word))\n\nclean_df_count=clean_df %>%  \n  count(word, sort = TRUE) %>%\n  top_n(100) %>%\n  ungroup\npaged_table(clean_df_count)"},{"path":"ielts-reading-topics.html","id":"topic-modeling-with-quanteda-stm","chapter":"IELTS Reading Topics","heading":"Topic modeling with quanteda & stm","text":"","code":""},{"path":"ielts-reading-topics.html","id":"document-feature-matrix","chapter":"IELTS Reading Topics","heading":"Document-feature matrix","text":"\nLet’s get started topic model! really fan stm package excellent results gotten experimenting . stm() function take input dfm quanteda.\n","code":"\nlibrary(quanteda)\nlibrary(stm)\nlibrary(ggdendro)\nlibrary(topicmodels)\n\nclean_df_dfm <- clean_df %>% \n  mutate(header=paste(book,test,passage,header)) %>%\n  count(header, word, sort = TRUE) %>%\n  cast_dfm(header, word, n)\n\nclean_df_dfm[1:6,1:6]#> Document-feature matrix of: 6 documents, 6 features (80.56% sparse) and 0 docvars.\n#>                               features\n#> docs                           play glass silk soil vehicl collect\n#>   14 3 3 The power of play       53     0    0    0      0       0\n#>   12 4 1 The History of Glass     0    38    0    0      0       0\n#>   11 3 1 The story of silk        0     0   36    0      0       2\n#>   13 4 2 SAVING THE SOIL          0     0    0   34      0       0\n#>   15 1 2 Driverless cars          0     0    0    0     28       0\n#>   12 1 2 COLLECTING AS A HOBBY    0     0    0    0      0      27"},{"path":"ielts-reading-topics.html","id":"dendrogram","chapter":"IELTS Reading Topics","heading":"Dendrogram","text":"\nsimple way word cluster analysis dendrogram term-document matrix.\nTDM, can call dist() compute differences row matrix.\n\nNext, call hclust() perform cluster analysis dissimilarities distance matrix. Lastly, can visualize word frequency distances using dendrogram plot(). Often text mining, can tease interesting insights word clusters based dendrogram.\n","code":"\ndistance=dist(as.matrix(clean_df_dfm), \n                   method = \"euclidean\")\nhc=hclust(d = distance, \n          method = \"complete\")\n\nggdendrogram(hc, rotate=TRUE, theme_dendro=FALSE)"},{"path":"ielts-reading-topics.html","id":"beta","chapter":"IELTS Reading Topics","heading":"Beta","text":"\nstm package summary() method trained topic models like print details screen, want get back tidy data frame can use dplyr ggplot2 data manipulation data visualization. can use tidy() output stm model, get probabilities word generated topic.\n’s nice stm package summary() method trained topic models like print details screen, 12 topics corresponding key words can’t combine , manipulate , even visualize themThat’s take look broom package, taking topics turning data frame using tidy function.Let’s tidy() beta matrix topic model look probabilities word generated topic.","code":"\nlibrary(stm)\ntopic_model <- stm(clean_df_dfm, K = 12)\ntopic_model#> A topic model with 12 topics, 60 documents and a 5161 word dictionary.\nsummary(topic_model)#> A topic model with 12 topics, 60 documents and a 5161 word dictionary.#> Topic 1 Top Words:\n#>       Highest Prob: theori, peopl, stori, happi, implicit, intellig, view \n#>       FREX: implicit, bentham, tehrani, wolf, happi, grandmoth, hood \n#>       Lift: folklorist, grandmoth, hamiltonian, hood, implicit, jeffersonian, prison \n#>       Score: implicit, intellig, happi, bentham, tehrani, theori, wolf \n#> Topic 2 Top Words:\n#>       Highest Prob: play, children, employe, child, develop, motiv, learn \n#>       FREX: al, categori, fun, hotel, employe, play, child \n#>       Lift: hrm, proposit, two thousand nine, whitebread, accumul, adaptabl, advice \n#>       Score: play, children, employe, al, child, categori, fun \n#> Topic 3 Top Words:\n#>       Highest Prob: collect, boredom, photograph, peopl, henderson, collector, feel \n#>       FREX: photograph, henderson, railway, boredom, collect, hobbi, photographi \n#>       Lift: edinburgh, hobbi, indiffer, notman, ottawa, phone, photographi \n#>       Score: boredom, photograph, henderson, collect, collector, aimless, railway \n#> Topic 4 Top Words:\n#>       Highest Prob: glass, build, cork, design, short, air, materi \n#>       FREX: cork, bottl, glass, ventil, build, air, short \n#>       Lift: miasma, ventil, adding, advancement, airstream, alan, alentejo \n#>       Score: glass, cork, bottl, adding, ventil, build, hospit \n#> Topic 5 Top Words:\n#>       Highest Prob: vehicl, product, food, increas, time, zealand, reduc \n#>       FREX: zealand, vehicl, tourism, websit, autom, board, organis \n#>       Lift: committe, embrace, essay, essayist, fan, greenhous, indoor \n#>       Score: vehicl, zealand, autom, tourism, websit, board, automot \n#> Topic 6 Top Words:\n#>       Highest Prob: soil, water, twin, peopl, world, popul, health \n#>       FREX: twin, geographi, moai, epigenet, soil, healthcar, lipo \n#>       Lift: desalin, harappan, nui, nurtur, petri, abject, abrupt \n#>       Score: soil, twin, geographi, moai, water, epigenet, popul \n#> Topic 7 Top Words:\n#>       Highest Prob: engineer, bike, citi, design, lift, scheme, geo \n#>       FREX: geo, schimmelpennink, hull, rose, gondola, bingham, bike \n#>       Lift: bolt, deposit, fietsenplan, fix, incas, instant, juli \n#>       Score: bike, engineer, geo, lift, schimmelpennink, scheme, gondola \n#> Topic 8 Top Words:\n#>       Highest Prob: music, sound, film, human, paint, audienc, comput \n#>       FREX: music, film, fool, audienc, comput, dialogu, colton \n#>       Lift: colton, emi, aaron, absurditi, acknowledg, actor, annoy \n#>       Score: music, film, sound, absurditi, dialogu, comput, fool \n#> Topic 9 Top Words:\n#>       Highest Prob: mathemat, public, moor, busi, sculptur, market, human \n#>       FREX: moor, sculptur, reader, mathemat, public, explorat, sculptor \n#>       Lift: algebra, arithmet, caveman, detect, enforc, era, firefight \n#>       Score: mathemat, moor, sculptur, reader, aficionado, law, principl \n#> Topic 10 Top Words:\n#>       Highest Prob: languag, studi, brain, sound, word, babi, peopl \n#>       FREX: bilingu, oxytocin, whistl, laughter, debri, activat, infant \n#>       Lift: silbador, à, abbey, absurd, accident, activat, adolesc \n#>       Score: languag, babi, bilingu, brain, ants, sound, oxytocin \n#> Topic 11 Top Words:\n#>       Highest Prob: speci, animal, compound, tortois, insect, zoo, migrat \n#>       FREX: tortois, zoo, pigeon, lynx, captiv, mammoth, reintroduc \n#>       Lift: antimicrobi, bred, conservationist, ear, thylacin, aberystwyth, abundant \n#>       Score: tortois, animal, compound, zoo, speci, migrat, extinct \n#> Topic 12 Top Words:\n#>       Highest Prob: silk, ship, coconut, trade, centuri, world, island \n#>       FREX: silk, coconut, cinnamon, nutmeg, spice, cutti, sark \n#>       Lift: caribbean, ceylon, fibr, flower, legend, portugues, seedl \n#>       Score: silk, coconut, cinnamon, nutmeg, spice, ship, cutti\nlibrary(broom)\n\ntd_beta=tidy(topic_model, matrix=\"beta\")\n\ntd_beta %>%\n    group_by(topic) %>%\n    top_n(10, beta) %>%\n    ungroup() %>%\n    mutate(topic = paste0(\"Topic \", topic),\n           term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(term, beta, fill = as.factor(topic))) +\n    geom_col(alpha = 0.8, show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free_y\") +\n    coord_flip() +\n    scale_x_reordered() +\n    labs(x = NULL, y = expression(beta),\n         title = \"Highest word probabilities for each topic\",\n         subtitle = \"Different words are associated with different topics\")"},{"path":"ielts-reading-topics.html","id":"gamma","chapter":"IELTS Reading Topics","heading":"Gamma","text":"\n’m also quite interested probabilities document generated topic, gamma matrix.\n\ntopic modeling process great example kind workflow often use text tidy data principles.\n\nuse tidy tools like dplyr, tidyr, ggplot2 initial data exploration preparation.\n\ncast non-tidy structure perform machine learning algorithm.\n\ntidy results statistical modeling can use tidy data principles understand model results.\n","code":"\ntd_gamma <- tidy(topic_model, matrix = \"gamma\",                    \n                 document_names = rownames(clean_df_dfm))\n\ntd_gamma %>% \n  filter(gamma>0.5) %>% \n  count(topic,sort=TRUE)#> # A tibble: 12 × 2\n#>    topic     n\n#>    <int> <int>\n#>  1    10     9\n#>  2     5     6\n#>  3     6     6\n#>  4     7     6\n#>  5    11     6\n#>  6    12     6\n#>  7     9     5\n#>  8     2     4\n#>  9     1     3\n#> 10     3     3\n#> 11     4     3\n#> 12     8     3\ntop_terms <- td_beta %>%\n  arrange(beta) %>%\n  group_by(topic) %>%\n  top_n(8, beta) %>%\n  arrange(-beta) %>%\n  select(topic, term) %>%\n  summarise(terms = list(term)) %>%\n  mutate(terms = map(terms, paste, collapse = \", \")) %>% \n  unnest()\n\ngamma_terms <- td_gamma %>%\n  group_by(topic) %>%\n  summarise(gamma = mean(gamma)) %>%\n  arrange(desc(gamma)) %>%\n  left_join(top_terms, by = \"topic\") %>%\n  mutate(topic = paste0(\"Topic \", topic),\n         topic = reorder(topic, gamma))\n\ngamma_terms %>%\n  ggplot(aes(topic, gamma, label = terms, fill = topic)) +\n  geom_text(aes(label = terms), vjust = 0.3,hjust=-0.1) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() + scale_y_continuous(limits=c(0,0.4)) +\n  theme_classic()"},{"path":"baby-names---sql.html","id":"baby-names---sql","chapter":"Baby Names - SQL","heading":"Baby Names - SQL","text":"\nDataCamp project scrape HTML elements platform personal workspace. included portfolio show comfortable SQL queries.\n","code":""},{"path":"baby-names---sql.html","id":"classic-american-names","chapter":"Baby Names - SQL","heading":"Classic American names","text":"\nPhoto Travis Wise Wikimedia.\n\nAmerican baby name tastes changed since 1920? names remained popular 100 years, names compare recent top baby names? considerations many new parents, skills ’ll practice answering queries broadly applicable. , understanding trends popularity important many businesses, !\n\n’ll working data provided United States Social Security Administration, lists first names along number sex babies given year. processing speed purposes, ’ve limited dataset first names given 5,000 American babies given year. data spans 101 years, 1920 2020.\n\nLet’s get oriented American baby name tastes looking names stood test time!\n\nFind names given 5,000 babies either sex every year 101 years 1920 2020; recall names show dataset least 5,000 babies given name year.\n\nSelect first_name total number babies ever given name.\n\nGroup first_name filter names appear 101 years.\n\nOrder results total number babies ever given name, descending.\n\nline postgresql:///names used connect database; don’t remove .\n\nproject uses techniques learned Intermediate SQL, including CASE statements, pattern matching using LIKE operator, subqueries, common table expressions, window functions. ’ll also expected know concepts Introduction SQL Joining Data SQL, select columns table, filter rows meet criterion, use aggregation functions, perform calculations groups rows, filter grouped data, join data.\n","code":"\nlibrary(tidyquery)\nlibrary(sqldf)\nbaby_names = read.csv(\"https://raw.githubusercontent.com/cliex159/DataCampcsv/main/usa_baby_names.csv\")\nquery(\"SELECT first_name, SUM(num)\nFROM baby_names\nGROUP BY first_name\nHAVING COUNT(year) = 101\nORDER BY SUM(num) DESC;\")#> # A tibble: 8 × 2\n#>   first_name `sum(num, na.rm = TRUE)`\n#>   <chr>                         <int>\n#> 1 James                       4748138\n#> 2 John                        4510721\n#> 3 William                     3614424\n#> 4 David                       3571498\n#> 5 Joseph                      2361382\n#> 6 Thomas                      2166802\n#> 7 Charles                     2112352\n#> 8 Elizabeth                   1436286"},{"path":"baby-names---sql.html","id":"timeless-or-trendy","chapter":"Baby Names - SQL","heading":"Timeless or trendy?","text":"\nWow, looks like lot timeless traditionally male names! Elizabeth holding female names, .\n\nNow, let’s broaden understanding dataset looking names. ’ll attempt capture type popularity name dataset enjoyed. name classic popular across many years trendy, popular years? Let’s find .\n\nClassify name’s popularity according number years name appears dataset.\n\nSelect first_name, sum babies ’ve ever given name, popularity_type.\n\nClassify names dataset ‘Classic,’ ‘Semi-classic,’ ‘Semi-trendy,’ ‘Trendy’ based whether name appears dataset 80, 50, 20, 0 times, respectively.\n\nAlias new classification column popularity_type.\n\nOrder results alphabetically first_name.\n","code":"\n# -- Classify first names as 'Classic', 'Semi-classic', 'Semi-trendy', or 'Trendy' \n# -- Alias this column as popularity_type \n# -- Select first_name, the sum of babies who have ever had that name, and popularity_type \n# -- Order the results alphabetically by first_name \n\nquery(\"SELECT first_name, SUM(num),\n    CASE WHEN COUNT(year) > 80 THEN 'Classic'\n        WHEN COUNT(year) > 50 THEN 'Semi-classic'\n        WHEN COUNT(year) > 20 THEN 'Semi-trendy'\n        ELSE 'Trendy' END AS popularity_type\nFROM baby_names\nGROUP BY first_name\nORDER BY first_name;\")#> # A tibble: 547 × 3\n#>    first_name `sum(num, na.rm = TRUE)` popularity_type\n#>    <chr>                         <int> <chr>          \n#>  1 Aaliyah                       15870 Trendy         \n#>  2 Aaron                        530592 Semi-classic   \n#>  3 Abigail                      338485 Semi-trendy    \n#>  4 Adam                         497293 Semi-trendy    \n#>  5 Addison                      107433 Trendy         \n#>  6 Adrian                       147741 Semi-trendy    \n#>  7 Aidan                         68566 Trendy         \n#>  8 Aiden                        216194 Trendy         \n#>  9 Alan                         162041 Semi-trendy    \n#> 10 Albert                       260945 Semi-trendy    \n#> # … with 537 more rows"},{"path":"baby-names---sql.html","id":"top-ranked-female-names-since-1920","chapter":"Baby Names - SQL","heading":"Top-ranked female names since 1920","text":"\nfind favorite American celebrity’s name popularity chart? classic trendy? think name Henry ? Jaxon?\n\nSince didn’t get many traditionally female names classic American names search first task, let’s limit search names given female babies.\n\ncan use opportunity practice window functions assigning rank female names based number babies ever given name. top-ranked female names since 1920?\n\nLet’s take look ten highest-ranked American female names dataset.\n\nSelect name_rank, first_name, sum babies ’ve ever name.\n\nRANK first_name sum babies ’ve ever name, aliasing name_rank showing names descending order name_rank.\n\nFilter data include results sex equals ‘F’.\n\nLimit ten results.\n","code":"\n# -- RANK names by the sum of babies who have ever had that name (descending), aliasing as name_rank \n# -- Select name_rank, first_name, and the sum of babies who have ever had that name \n# -- Filter the data for results where sex equals 'F' \n# -- Limit to ten results \nsqldf(\"\nSELECT\n    RANK() OVER(ORDER BY SUM(num) DESC) AS name_rank,\n    first_name, SUM(num)\nFROM baby_names\nWHERE sex = 'F'\nGROUP BY first_name\nLIMIT 10;\")#>    name_rank first_name SUM(num)\n#> 1          1       Mary  3215850\n#> 2          2   Patricia  1479802\n#> 3          3  Elizabeth  1436286\n#> 4          4   Jennifer  1404743\n#> 5          5      Linda  1361021\n#> 6          6    Barbara  1343901\n#> 7          7      Susan  1025728\n#> 8          8    Jessica   994210\n#> 9          9       Lisa   920119\n#> 10        10      Betty   893396"},{"path":"baby-names---sql.html","id":"picking-a-baby-name","chapter":"Baby Names - SQL","heading":"Picking a baby name","text":"\nPerhaps friend heard work analyzing baby names like help choosing name baby, girl. doesn’t like top-ranked names found previous task.\n\n’s set traditionally female name ending letter ‘’ since ’s heard vowels baby names trendy. ’s also looking name popular years since 2015.\n\nLet’s see can find options friend!\n\nReturn list first names meet friend’s baby name criteria.\n\nSelect first_name column.\n\nFilter data results sex equals ‘F’, year greater 2015, first_name ends ‘.’\n\nGroup data first_name order total number babies ever given first_name, descending.\n","code":"\n# -- Select only the first_name column \n# -- Filter for results where sex is 'F', year is greater than 2015, and first_name ends in 'a' \n# -- Group by first_name and order by the total number of babies given that first_name \nquery(\"SELECT first_name\nFROM baby_names\nWHERE sex = 'F' AND year > 2015\n    AND first_name LIKE '%a'\nGROUP BY first_name\nORDER BY SUM(num) DESC;\")#> # A tibble: 19 × 1\n#>    first_name\n#>    <chr>     \n#>  1 Olivia    \n#>  2 Emma      \n#>  3 Ava       \n#>  4 Sophia    \n#>  5 Isabella  \n#>  6 Mia       \n#>  7 Amelia    \n#>  8 Ella      \n#>  9 Sofia     \n#> 10 Camila    \n#> 11 Aria      \n#> 12 Victoria  \n#> 13 Layla     \n#> 14 Nora      \n#> 15 Mila      \n#> 16 Luna      \n#> 17 Stella    \n#> 18 Gianna    \n#> 19 Aurora"},{"path":"baby-names---sql.html","id":"the-olivia-expansion","chapter":"Baby Names - SQL","heading":"The Olivia expansion","text":"\nBased results previous task, can see Olivia popular female name ending ‘’ since 2015. name Olivia become popular?\n\nLet’s explore rise name Olivia help window function.\n\nFind cumulative number babies named Olivia years since name first appeared dataset.\n\nSelect year, first_name, num Olivias year, cumulative_olivias.\n\nUsing window function, sum cumulative number babies ever named Olivia year; alias cumulative_olivias.\n\nFilter results data name Olivia returned.\n\nOrder results year earliest year Olivia appeared dataset recent.\n","code":"\n# -- Select year, first_name, num of Olivias in that year, and cumulative_olivias \n# -- Sum the cumulative babies who have been named Olivia up to that year; alias as cumulative_olivias \n# -- Filter so that only data for the name Olivia is returned. \n# -- Order by year from the earliest year to most recent \n\nsqldf(\"SELECT year, first_name, num,\n    SUM(num) OVER (ORDER BY year) AS cumulative_olivias\nFROM baby_names\nWHERE first_name = 'Olivia'\nORDER BY year;\")#>    year first_name   num cumulative_olivias\n#> 1  1991     Olivia  5601               5601\n#> 2  1992     Olivia  5809              11410\n#> 3  1993     Olivia  6340              17750\n#> 4  1994     Olivia  6434              24184\n#> 5  1995     Olivia  7624              31808\n#> 6  1996     Olivia  8124              39932\n#> 7  1997     Olivia  9477              49409\n#> 8  1998     Olivia 10610              60019\n#> 9  1999     Olivia 11255              71274\n#> 10 2000     Olivia 12852              84126\n#> 11 2001     Olivia 13977              98103\n#> 12 2002     Olivia 14630             112733\n#> 13 2003     Olivia 16152             128885\n#> 14 2004     Olivia 16106             144991\n#> 15 2005     Olivia 15694             160685\n#> 16 2006     Olivia 15501             176186\n#> 17 2007     Olivia 16584             192770\n#> 18 2008     Olivia 17084             209854\n#> 19 2009     Olivia 17438             227292\n#> 20 2010     Olivia 17029             244321\n#> 21 2011     Olivia 17327             261648\n#> 22 2012     Olivia 17320             278968\n#> 23 2013     Olivia 18439             297407\n#> 24 2014     Olivia 19823             317230\n#> 25 2015     Olivia 19710             336940\n#> 26 2016     Olivia 19380             356320\n#> 27 2017     Olivia 18744             375064\n#> 28 2018     Olivia 18011             393075\n#> 29 2019     Olivia 18508             411583\n#> 30 2020     Olivia 17535             429118"},{"path":"baby-names---sql.html","id":"many-males-with-the-same-name","chapter":"Baby Names - SQL","heading":"Many males with the same name","text":"\nWow, Olivia meteoric rise! Let’s take look traditionally male names now. saw first task nine traditionally male names given least 5,000 babies every single year 101-year dataset! names classics, showing dataset every year doesn’t necessarily mean timeless names popular. Let’s explore popular male names little .\n\nnext two tasks, build listing every year along popular male name year. presents common problem: find greatest X group? , context problem, find male name given highest number babies year?\n\nSQL, one approach use subquery. can first write query selects year maximum num babies given single male name year. example, 1989, male name given highest number babies given 65,339 babies. ’ll write query task. next task, can use code task subquery look first_name given 65,339 babies 1989… well top male first name years!\n\nWrite query selects year maximum num babies given male name year.\n\nSelect year maximum num babies given one male name year; alias maximum max_num.\n\nFilter data include results sex equals ‘M’.\n","code":"\n# -- Select year and maximum number of babies given any one male name in that year, aliased as max_num \n# -- Filter the data to include only results where sex equals 'M' \n\nquery(\"SELECT year, MAX(num) AS max_num\nFROM baby_names\nWHERE sex = 'M'\nGROUP BY year;\")#> # A tibble: 101 × 2\n#>     year max_num\n#>    <int>   <int>\n#>  1  1920   56914\n#>  2  1921   58215\n#>  3  1922   57280\n#>  4  1923   57469\n#>  5  1924   60801\n#>  6  1925   60897\n#>  7  1926   61130\n#>  8  1927   61671\n#>  9  1928   60703\n#> 10  1929   59804\n#> # … with 91 more rows"},{"path":"baby-names---sql.html","id":"top-male-names-over-the-years","chapter":"Baby Names - SQL","heading":"Top male names over the years","text":"\nprevious task, found maximum number babies given one male name year. Incredibly, popular name year varied given less 20,000 babies given 90,000!\n\ntask, find top male name year dataset.\n\nUsing previous task’s code subquery, look first_name corresponds maximum number babies given specific male name year.\n\nSelect year, first_name given largest number male babies, num babies given first_name year.\n\nJoin baby_names code last task subquery, using whatever alias like joining columns subquery.\n\nOrder results year, starting recent year.\n","code":"\n# -- Select year, first_name given to the largest number of male babies, and num of babies given that name \n# -- Join baby_names to the code in the last task as a subquery \n# -- Order results by year descending \n\nsqldf(\"SELECT b.year, b.first_name, b.num\nFROM baby_names AS b\nINNER JOIN (\n    SELECT year, MAX(num) as max_num\n    FROM baby_names\n    WHERE sex = 'M'\n    GROUP BY year) AS subquery \nON subquery.year = b.year \n    AND subquery.max_num = b.num\nORDER BY b.year DESC;\")#>     year first_name   num\n#> 1   2020       Liam 19659\n#> 2   2019       Liam 20555\n#> 3   2018       Liam 19924\n#> 4   2017       Liam 18824\n#> 5   2016       Noah 19154\n#> 6   2015       Noah 19650\n#> 7   2014       Noah 19319\n#> 8   2013       Noah 18266\n#> 9   2012      Jacob 19088\n#> 10  2011      Jacob 20378\n#> 11  2010      Jacob 22139\n#> 12  2009      Jacob 21184\n#> 13  2008      Jacob 22603\n#> 14  2007      Jacob 24292\n#> 15  2006      Jacob 24850\n#> 16  2005      Jacob 25837\n#> 17  2004      Jacob 27886\n#> 18  2003      Jacob 29643\n#> 19  2002      Jacob 30579\n#> 20  2001      Jacob 32554\n#> 21  2000      Jacob 34483\n#> 22  1999      Jacob 35367\n#> 23  1998    Michael 36616\n#> 24  1997    Michael 37549\n#> 25  1996    Michael 38365\n#> 26  1995    Michael 41399\n#> 27  1994    Michael 44472\n#> 28  1993    Michael 49554\n#> 29  1992    Michael 54397\n#> 30  1991    Michael 60793\n#> 31  1990    Michael 65302\n#> 32  1989    Michael 65399\n#> 33  1988    Michael 64150\n#> 34  1987    Michael 63654\n#> 35  1986    Michael 64224\n#> 36  1985    Michael 64924\n#> 37  1984    Michael 67745\n#> 38  1983    Michael 68010\n#> 39  1982    Michael 68244\n#> 40  1981    Michael 68776\n#> 41  1980    Michael 68704\n#> 42  1979    Michael 67742\n#> 43  1978    Michael 67157\n#> 44  1977    Michael 67609\n#> 45  1976    Michael 66947\n#> 46  1975    Michael 68451\n#> 47  1974    Michael 67580\n#> 48  1973    Michael 67842\n#> 49  1972    Michael 71401\n#> 50  1971    Michael 77599\n#> 51  1970    Michael 85291\n#> 52  1969    Michael 85201\n#> 53  1968    Michael 81995\n#> 54  1967    Michael 82440\n#> 55  1966    Michael 79990\n#> 56  1965    Michael 81021\n#> 57  1964    Michael 82642\n#> 58  1963    Michael 83778\n#> 59  1962    Michael 85041\n#> 60  1961    Michael 86917\n#> 61  1960      David 85933\n#> 62  1959    Michael 85224\n#> 63  1958    Michael 90564\n#> 64  1957    Michael 92718\n#> 65  1956    Michael 90665\n#> 66  1955    Michael 88372\n#> 67  1954    Michael 88576\n#> 68  1953     Robert 86247\n#> 69  1952      James 87063\n#> 70  1951      James 87261\n#> 71  1950      James 86229\n#> 72  1949      James 86865\n#> 73  1948      James 88589\n#> 74  1947      James 94764\n#> 75  1946      James 87439\n#> 76  1945      James 74460\n#> 77  1944      James 76954\n#> 78  1943      James 80274\n#> 79  1942      James 77174\n#> 80  1941      James 66743\n#> 81  1940      James 62476\n#> 82  1939     Robert 59653\n#> 83  1938     Robert 62269\n#> 84  1937     Robert 61842\n#> 85  1936     Robert 58499\n#> 86  1935     Robert 56522\n#> 87  1934     Robert 55834\n#> 88  1933     Robert 54223\n#> 89  1932     Robert 59265\n#> 90  1931     Robert 60518\n#> 91  1930     Robert 62149\n#> 92  1929     Robert 59804\n#> 93  1928     Robert 60703\n#> 94  1927     Robert 61671\n#> 95  1926     Robert 61130\n#> 96  1925     Robert 60897\n#> 97  1924     Robert 60801\n#> 98  1923       John 57469\n#> 99  1922       John 57280\n#> 100 1921       John 58215\n#> 101 1920       John 56914"},{"path":"baby-names---sql.html","id":"the-most-years-at-number-one","chapter":"Baby Names - SQL","heading":"The most years at number one","text":"\nNoah Liam ruled roost last years, scroll results, looks like Michael Jacob also spent good number years top name! name number one largest number years? Let’s use common table expression find .\n\nReturn list first names top male first name year along count number years name top name.\n\nSelect first_name count number years first_name appeared year’s top name last task; alias count count_top_name.\n\n, use code previous task common table expression.\n\nGroup first_name order results name years top name fewest.\n","code":"\n# -- Select first_name and a count of years it was the top name in the last task; alias as count_top_name \n# -- Use the code from the previous task as a common table expression \n# -- Group by first_name and order by count_top_name descending \n\nsqldf(\"WITH top_male_names AS (\n    SELECT b.year, b.first_name, b.num\n    FROM baby_names AS b\n    INNER JOIN (\n        SELECT year, MAX(num) num\n        FROM baby_names\n        WHERE sex = 'M'\n        GROUP BY year) AS subquery \n    ON subquery.year = b.year \n        AND subquery.num = b.num\n    ORDER BY subquery.YEAR DESC\n    )\nSELECT first_name, COUNT(first_name) as count_top_name\nFROM top_male_names\nGROUP BY first_name\nORDER BY COUNT(first_name) DESC;\")#>   first_name count_top_name\n#> 1    Michael             44\n#> 2     Robert             17\n#> 3      Jacob             14\n#> 4      James             13\n#> 5       Noah              4\n#> 6       Liam              4\n#> 7       John              4\n#> 8      David              1"}]
